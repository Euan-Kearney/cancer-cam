{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Euan-Kearney/cancer-cam/blob/main/model_0_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstalls existing kaggle files to resolve potential API issues\n",
        "!pip uninstall -y kaggle kagglesdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il1b0lT02NlI",
        "outputId": "48e481a4-e134-4aff-ed97-eef84e0851b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: kaggle 1.7.4.5\n",
            "Uninstalling kaggle-1.7.4.5:\n",
            "  Successfully uninstalled kaggle-1.7.4.5\n",
            "\u001b[33mWARNING: Skipping kagglesdk as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install -q kagglesdk\n",
        "pip install -q kaggle\n",
        "pip install -q opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqWxdst2RHz",
        "outputId": "04b6f66f-b617-44f8-9320-1a25ef16db7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvWzzqnn693s",
        "outputId": "ac2a0cc5-115d-4a90-8fa0-eeeb3ad03a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000\n",
            "License(s): CC-BY-NC-SA-4.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Retrieve kaggle key and import json\n",
        "kaggle_json_str = userdata.get('KAGGLE_JSON')\n",
        "kaggle_dict = json.loads(kaggle_json_str)\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_dict, f)\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000 -p /content/HAM10000 --unzip --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "54kvhyvRz3Vg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "base = 'HAM10000_organised'\n",
        "splits = ['train', 'validate', 'test']\n",
        "os.makedirs(base, exist_ok=True)\n",
        "\n",
        "classes = [\n",
        "    'melanoma',\n",
        "    'BCC',\n",
        "    'SCC',\n",
        "    'low_risk'\n",
        "]\n",
        "\n",
        "# Setup organised directories to allow for the creation of dataframes\n",
        "for split in splits:\n",
        "    for cls in classes:\n",
        "        path = os.path.join(base, split, cls)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "os.makedirs(base, exist_ok=True)\n",
        "\n",
        "\"\"\"\n",
        "Maps HAM10000 classes to the 4 cancer-cam classes\n",
        "\"\"\"\n",
        "def map_classes(cls):\n",
        "  if cls == 'mel':\n",
        "    return 'melanoma'\n",
        "  elif cls == 'bcc':\n",
        "    return 'BCC'\n",
        "  elif cls == 'akiec':\n",
        "    return 'SCC'\n",
        "  elif cls == 'bkl':\n",
        "    return 'low_risk'\n",
        "  elif cls == 'df':\n",
        "    return 'low_risk'\n",
        "  elif cls == 'nv':\n",
        "    return 'low_risk'\n",
        "  elif cls == 'vasc':\n",
        "    return 'low_risk'\n",
        "\"\"\"\n",
        "Copies images from HAM10000 folder to the HAM10000_organised folder\n",
        "\"\"\"\n",
        "def copy_images(split, dataframe):\n",
        "  for index, row in dataframe.iterrows():\n",
        "    image_name = row['image_id'] + '.jpg'\n",
        "    cls = map_classes(row['dx'])\n",
        "    part_1_source = os.path.join('HAM10000/HAM10000_images_part_1/', image_name)\n",
        "    part_2_source = os.path.join('HAM10000/HAM10000_images_part_2/', image_name)\n",
        "    destination = os.path.join('HAM10000_organised/', split, cls, image_name)\n",
        "    # Checks if image already exists in new dir, ceasing the for loop if so\n",
        "    if os.path.exists(destination):\n",
        "      break\n",
        "    # Checks if the image exists in the part_1 HAM10000 dir, or the part_2 folder\n",
        "    if os.path.exists(part_1_source):\n",
        "      shutil.copyfile(part_1_source, destination)\n",
        "    elif os.path.exists(part_2_source):\n",
        "      shutil.copyfile(part_2_source, destination)\n",
        "\n",
        "all_data = pd.read_csv('HAM10000/HAM10000_metadata.csv')\n",
        "train_df, temp_df = train_test_split(all_data, test_size=0.2)\n",
        "validate_df, test_df = train_test_split(temp_df, test_size=0.5)\n",
        "\n",
        "for split in splits:\n",
        "  if split == 'train':\n",
        "    copy_images(split, train_df)\n",
        "  elif split == 'validate':\n",
        "    copy_images(split, validate_df)\n",
        "  else:\n",
        "    copy_images(split, test_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CKS70nkKXEh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05409cc4-8b0c-4614-d5e5-bb7907f2fccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3 directories and 0 images in 'HAM10000_organised/'.\n",
            "There are 4 directories and 0 images in 'HAM10000_organised/test'.\n",
            "There are 0 directories and 46 images in 'HAM10000_organised/test/BCC'.\n",
            "There are 0 directories and 31 images in 'HAM10000_organised/test/SCC'.\n",
            "There are 0 directories and 817 images in 'HAM10000_organised/test/low_risk'.\n",
            "There are 0 directories and 108 images in 'HAM10000_organised/test/melanoma'.\n",
            "There are 4 directories and 0 images in 'HAM10000_organised/train'.\n",
            "There are 0 directories and 410 images in 'HAM10000_organised/train/BCC'.\n",
            "There are 0 directories and 268 images in 'HAM10000_organised/train/SCC'.\n",
            "There are 0 directories and 6437 images in 'HAM10000_organised/train/low_risk'.\n",
            "There are 0 directories and 897 images in 'HAM10000_organised/train/melanoma'.\n",
            "There are 4 directories and 0 images in 'HAM10000_organised/validate'.\n",
            "There are 0 directories and 58 images in 'HAM10000_organised/validate/BCC'.\n",
            "There are 0 directories and 28 images in 'HAM10000_organised/validate/SCC'.\n",
            "There are 0 directories and 807 images in 'HAM10000_organised/validate/low_risk'.\n",
            "There are 0 directories and 108 images in 'HAM10000_organised/validate/melanoma'.\n"
          ]
        }
      ],
      "source": [
        "# Verifies number of images in each dir\n",
        "def count_images(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "count_images('HAM10000_organised/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Removes non image files from image directories\n",
        "!rm -R HAM10000_organised/train/.ipynb_checkpoints\n",
        "!rm -R HAM10000_organised/validate/.ipynb_checkpoints\n",
        "!rm -R HAM10000_organised/test/.ipynb_checkpoints\n",
        "\n",
        "# Lighweight B0 model has its weight's and image transformations retrieved\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "auto_transforms = weights.transforms()\n",
        "\n",
        "train_data = datasets.ImageFolder(\"HAM10000_organised/train\", transform=auto_transforms)\n",
        "validate_data = datasets.ImageFolder(\"HAM10000_organised/validate\", transform=auto_transforms)\n",
        "test_data = datasets.ImageFolder(\"HAM10000_organised/test\", transform=auto_transforms)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "validate_dataloader = DataLoader(\n",
        "    validate_data,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=62,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o-_5vcYYe-qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7a5dcf-7946-401a-8e42-58ff02c49b03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'HAM10000_organised/train/.ipynb_checkpoints': No such file or directory\n",
            "rm: cannot remove 'HAM10000_organised/validate/.ipynb_checkpoints': No such file or directory\n",
            "rm: cannot remove 'HAM10000_organised/test/.ipynb_checkpoints': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses GPU for training if available, otherwise uses CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
      ],
      "metadata": {
        "id": "tPLKmd0jfdZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765c3561-c7d9-431e-ffe5-047a10f0cfe1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 128MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Freeze existing layers to ensure they remain unaffected by further training\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=4,\n",
        "                    bias=True)).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "oVQKTy8j_X9U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XqjBLzr3sRtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv4hVnaKYOsJYgqhHtODZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}