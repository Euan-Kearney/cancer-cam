{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Euan-Kearney/cancer-cam/blob/main/model_0_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstalls existing kaggle files to resolve potential API issues\n",
        "!pip uninstall -y kaggle kagglesdk"
      ],
      "metadata": {
        "id": "Il1b0lT02NlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install -q kagglesdk\n",
        "pip install -q kaggle\n",
        "pip install -q opendatasets"
      ],
      "metadata": {
        "id": "UIqWxdst2RHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvWzzqnn693s"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Retrieve kaggle key and import json\n",
        "kaggle_json_str = userdata.get('KAGGLE_JSON')\n",
        "kaggle_dict = json.loads(kaggle_json_str)\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_dict, f)\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000 -p /content/HAM10000 --unzip --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54kvhyvRz3Vg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "base = 'HAM10000_organised'\n",
        "splits = ['train', 'validate', 'test']\n",
        "os.makedirs(base, exist_ok=True)\n",
        "\n",
        "classes = [\n",
        "    'melanoma',\n",
        "    'BCC',\n",
        "    'SCC',\n",
        "    'low_risk'\n",
        "]\n",
        "\n",
        "# Setup organised directories to allow for the creation of dataframes\n",
        "for split in splits:\n",
        "    for cls in classes:\n",
        "        path = os.path.join(base, split, cls)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "os.makedirs(base, exist_ok=True)\n",
        "\n",
        "\"\"\"\n",
        "Maps HAM10000 classes to the 4 cancer-cam classes\n",
        "\"\"\"\n",
        "def map_classes(cls):\n",
        "  if cls == 'mel':\n",
        "    return 'melanoma'\n",
        "  elif cls == 'bcc':\n",
        "    return 'BCC'\n",
        "  elif cls == 'akiec':\n",
        "    return 'SCC'\n",
        "  elif cls == 'bkl':\n",
        "    return 'low_risk'\n",
        "  elif cls == 'df':\n",
        "    return 'low_risk'\n",
        "  elif cls == 'nv':\n",
        "    return 'low_risk'\n",
        "  elif cls == 'vasc':\n",
        "    return 'low_risk'\n",
        "\"\"\"\n",
        "Copies images from HAM10000 folder to the HAM10000_organised folder\n",
        "\"\"\"\n",
        "def copy_images(split, dataframe):\n",
        "  for index, row in dataframe.iterrows():\n",
        "    image_name = row['image_id'] + '.jpg'\n",
        "    cls = map_classes(row['dx'])\n",
        "    part_1_source = os.path.join('HAM10000/HAM10000_images_part_1/', image_name)\n",
        "    part_2_source = os.path.join('HAM10000/HAM10000_images_part_2/', image_name)\n",
        "    destination = os.path.join('HAM10000_organised/', split, cls, image_name)\n",
        "    # Checks if image already exists in new dir, ceasing the for loop if so\n",
        "    if os.path.exists(destination):\n",
        "      break\n",
        "    # Checks if the image exists in the part_1 HAM10000 dir, or the part_2 folder\n",
        "    if os.path.exists(part_1_source):\n",
        "      shutil.copyfile(part_1_source, destination)\n",
        "    elif os.path.exists(part_2_source):\n",
        "      shutil.copyfile(part_2_source, destination)\n",
        "\n",
        "all_data = pd.read_csv('HAM10000/HAM10000_metadata.csv')\n",
        "train_df, temp_df = train_test_split(all_data, test_size=0.2, random_state=22)\n",
        "validate_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=22)\n",
        "\n",
        "for split in splits:\n",
        "  if split == 'train':\n",
        "    copy_images(split, train_df)\n",
        "  elif split == 'validate':\n",
        "    copy_images(split, validate_df)\n",
        "  else:\n",
        "    copy_images(split, test_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKS70nkKXEh1"
      },
      "outputs": [],
      "source": [
        "# Verifies number of images in each dir\n",
        "def count_images(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "count_images('HAM10000_organised/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Removes non image files from image directories\n",
        "!rm -R HAM10000_organised/train/.ipynb_checkpoints\n",
        "!rm -R HAM10000_organised/validate/.ipynb_checkpoints\n",
        "!rm -R HAM10000_organised/test/.ipynb_checkpoints\n",
        "\n",
        "# Lighweight B0 model has its weight's and image transformations retrieved\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "auto_transforms = weights.transforms()\n",
        "\n",
        "train_data = datasets.ImageFolder(\"HAM10000_organised/train\", transform=auto_transforms)\n",
        "validate_data = datasets.ImageFolder(\"HAM10000_organised/validate\", transform=auto_transforms)\n",
        "test_data = datasets.ImageFolder(\"HAM10000_organised/test\", transform=auto_transforms)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "validate_dataloader = DataLoader(\n",
        "    validate_data,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=62,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(validate_dataloader.dataset.class_to_idx)\n",
        "\n"
      ],
      "metadata": {
        "id": "o-_5vcYYe-qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses GPU for training if available, otherwise uses CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
      ],
      "metadata": {
        "id": "tPLKmd0jfdZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Freeze existing layers to ensure they remain unaffected by further training\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "torch.manual_seed(22)\n",
        "torch.cuda.manual_seed(22)\n",
        "\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=4,\n",
        "                    bias=True)).to(device)\n",
        "\n",
        "class_weights = torch.tensor([4.897, 7.645, 0.3106, 2.246], dtype=torch.float)\n",
        "class_weights = class_weights.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "oVQKTy8j_X9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "train_losses, train_accs, val_losses, val_accs, val_f1s = [], [], [], [], []\n",
        "\n",
        "def train_epoch():\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for inputs, labels in train_dataloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    running_loss = loss.item() * inputs.size(0)\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "  epoch_loss = running_loss / total\n",
        "  epoch_acc = 100. * correct / total\n",
        "\n",
        "  train_losses.append(epoch_loss)\n",
        "  train_accs.append(epoch_acc)\n",
        "\n",
        "  return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch():\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validate_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_preds.append(predicted.detach().cpu())\n",
        "            all_targets.append(labels.detach().cpu())\n",
        "\n",
        "    val_loss = running_loss / total\n",
        "    val_acc = 100. * correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "    macro_f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "    val_f1s.append(macro_f1)\n",
        "\n",
        "    return val_loss, val_acc, macro_f1\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(30)):\n",
        "  train_loss, train_acc = train_epoch()\n",
        "  val_loss, val_acc, val_f1 = validate_epoch()\n",
        "  print(f\"epoch: {epoch} training loss: {train_loss} train accuracy: {train_acc} validate loss: {val_loss} validate accuracy: {val_acc} validate f1: {val_f1}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XqjBLzr3sRtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Validate Accuracy')\n",
        "plt.title('Accuracy vs Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validate Loss')\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(val_f1s, label='Validate F1 Score')\n",
        "plt.title('Validate F1 Score vs Epoch')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "STbTFN1a8au2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "destination_dir  = '/content/drive/MyDrive/cancer_cam/models'\n",
        "model_path = os.path.join(destination_dir, 'model_0_c.pth')\n",
        "torch.save(model, model_path)"
      ],
      "metadata": {
        "id": "jbpAdAWl8Vgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOA6tlP/4dg5qGbAm5JHLPN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}